{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manuel Archila 161250\n",
    "##### Diego Franco 20240\n",
    "##### Juan Diego Avila 20090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.290354\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.315570\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.299078\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.298903\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.300027\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.312724\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.307347\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.314235\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.299103\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.297892\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300854\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.304880\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295555\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.306973\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.288721\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.300674\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.300735\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.296987\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.297271\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.303181\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.297223\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.307899\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.305202\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.299099\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.278854\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.305628\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.292988\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.284897\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.301391\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.292528\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.300167\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.318235\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.282169\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.308214\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.293639\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.287900\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.289978\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.289389\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.282530\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.280149\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.293365\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.291999\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.289127\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.285772\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.282268\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.272134\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.267790\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.270984\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.259552\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.279252\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.265924\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.263482\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.271164\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.255219\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.227378\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.241599\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.201749\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.225660\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.214654\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.207725\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.238866\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.173890\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.147619\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.129619\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.081799\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.104464\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.122292\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.079769\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.041947\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.935064\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.980747\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.821163\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.860492\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.967731\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.738807\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.842051\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.723595\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.634346\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.704315\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.696681\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.585786\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.460944\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.504637\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.538492\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.371103\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.488982\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.280876\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.339621\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.340497\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.295983\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.223771\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.143815\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.285403\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.114302\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.140909\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.214866\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.105295\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.114371\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.957275\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.113677\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.016166\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.836244\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.917692\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.962960\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.914223\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.935577\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.927864\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.908553\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.985946\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.797681\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.778887\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.921462\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.739085\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.915735\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.749777\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.927942\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.868798\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.705947\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.867870\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.877646\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.821957\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.677227\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.888611\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.587170\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.953355\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.589124\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.667878\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.710181\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.603574\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.790573\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.381692\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.691773\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.644380\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.597041\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.514326\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.674853\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.462263\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.619597\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.621748\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.470695\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.660009\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.599284\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.528506\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.608190\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.532200\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.468184\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.483214\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.643663\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.454628\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.467120\n",
      "(87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Definición del modelo\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(F.avg_pool2d(self.conv1(x), 2))\n",
    "        x = torch.tanh(F.avg_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Hiperparámetros\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 0.01\n",
    "\n",
    "# Transformaciones y carga del dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instancia del modelo, función de pérdida y optimizador\n",
    "model = LeNet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)} \"\n",
    "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluación\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "print(f\"({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Loss: 2.302589490895381\n",
      "Epoch 2/30, Loss: 2.3025704125309234\n",
      "Epoch 3/30, Loss: 2.3025254858729176\n",
      "Epoch 4/30, Loss: 2.302226912944823\n",
      "Epoch 5/30, Loss: 2.25119497983352\n",
      "Epoch 6/30, Loss: 2.213179388619445\n",
      "Epoch 7/30, Loss: 2.1572559105465783\n",
      "Epoch 8/30, Loss: 2.111590341533846\n",
      "Epoch 9/30, Loss: 2.071355868635885\n",
      "Epoch 10/30, Loss: 2.041391796163281\n",
      "Epoch 11/30, Loss: 1.9850449238896675\n",
      "Epoch 12/30, Loss: 1.9442216006996076\n",
      "Epoch 13/30, Loss: 1.9191369117068513\n",
      "Epoch 14/30, Loss: 1.886217120815726\n",
      "Epoch 15/30, Loss: 1.8580890584479817\n",
      "Epoch 16/30, Loss: 1.833253474948961\n",
      "Epoch 17/30, Loss: 1.8073494653872517\n",
      "Epoch 18/30, Loss: 1.7937313595696178\n",
      "Epoch 19/30, Loss: 1.7717730893808252\n",
      "Epoch 20/30, Loss: 1.7616859060114303\n",
      "Epoch 21/30, Loss: 1.758428267048448\n",
      "Epoch 22/30, Loss: 1.7453365409770585\n",
      "Epoch 23/30, Loss: 1.732283755031693\n",
      "Epoch 24/30, Loss: 1.710803150216027\n",
      "Epoch 25/30, Loss: 1.705536770546223\n",
      "Epoch 26/30, Loss: 1.6985302739740942\n",
      "Epoch 27/30, Loss: 1.695783346967624\n",
      "Epoch 28/30, Loss: 1.6854584367988665\n",
      "Epoch 29/30, Loss: 1.678006345658656\n",
      "Epoch 30/30, Loss: 1.6738292587077832\n",
      "Accuracy on test set: 75.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # Capa convolucional 1\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.lrn1 = nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capa convolucional 2\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capa convolucional 3\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Capa convolucional 4\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Capa convolucional 5\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capas completamente conectadas\n",
    "        self.fc6 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc8 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.lrn1(self.relu1(self.conv1(x))))\n",
    "        x = self.pool2(self.lrn2(self.relu2(self.conv2(x))))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool5(self.relu5(self.conv5(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        \n",
    "        x = self.dropout6(self.relu6(self.fc6(x)))\n",
    "        x = self.dropout7(self.relu7(self.fc7(x)))\n",
    "        \n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=30):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy}%\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ¿Cuál es la diferencia principal entre ambas arquitecturas?\n",
    "\n",
    "- LaNet-5: es una arquitectura más simple, contando con 2 capas convolucionales y 3 capas completamente conectadas. Se utiliza la función de ctiación de tanh y existe un menor número de parámetros y menor profundidad que logra AlexNet.\n",
    "- AlexNet: es una arquitectura más compleja, que posee 5 capas convolucionales y 3 capas completamente conectadas. Utiliza la función de activación ReLU e implementa funciones como dropout para evitar el sobreajuste. Permite mayor profundiad y tiene un mayor número de parámetros que LaNet-5.\n",
    "\n",
    "b. Podría usarse LeNet-5 para un problema como el que resolvió usando AlexNet? ¿Y viceversa?\n",
    "- Tecnicamente LeNet-5 si se podría usar para problemas que resolvio Alexnet. Sin embargo LeNet-5 tiene un menor capaciddad y profundidad haciendo que no funcione tan bien como Alexnet. Por otro lado, Alexnet si se podría usar para problemas que resolvio LeNet-5, pero al tener una mayor capacidad y profundidad. Esta mayor capacidad puede hace que se use un modelo más complejo para problrmas más sencillos, lo cual puede llevar a un uso innecesario de recursos.\n",
    "\n",
    "c. Indique claramente qué le pareció más interesante de cada arquitectura\n",
    "- Lo que más nos llamó la antención de LeNet-5 es la simplicidad del modelo. A pesar de ser considerablemente simple, logró obtener resultados bastante buenos. Por otro lado, lo que más nos llamó la atención de Alexnet es la complejidad del modelo. A pesar de ser un modelo complejo, no logró resultados tan buenos como LeNet-5. Esto nos hace pensar que la complejidad de un modelo no necesariamente se traduce en mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigue e indique en qué casos son útiles las siguientes arquitecturas, agregue imagenes si esto le ayuda a una mejor comprensión\n",
    "\n",
    "a. GoogleNet (Inception)\n",
    "\n",
    "- GoogleNet, también conocida como Inception, es una arquitectura de CNN desarrollada por Google. Se destacó por su profundidad y eficiencia en la utilización de los recursos.\n",
    "- Es útil en casos donde se requieren redes profundas pero se desea mantener un uso eficiente de los recursos computacionales. GoogleNet utiliza una estructura llamada \"módulos Inception\" que combina múltiples tamaños de filtros de convolución en paralelo, permitiendo la extracción de características a diferentes escalas.\n",
    "- Útil para tareas de clasificación de imágenes, detección de objetos y segmentación semántica.\n",
    "\n",
    "b. DenseNet (Densely Connected Convolutional Networks)\n",
    "\n",
    "- DenseNet es una arquitectura de CNN que se caracteriza por su densa conectividad entre capas. Cada capa está conectada directamente con todas las capas subsiguientes.\n",
    "- Es útil en casos donde se desea un mejor flujo de información y gradientes más fuertes a lo largo de la red, lo que facilita el entrenamiento de redes profundas.\n",
    "- Útil para tareas de clasificación de imágenes, detección de objetos y segmentación semántica.\n",
    "\n",
    "c. MobileNet\n",
    "\n",
    "- MobileNet es una arquitectura de CNN diseñada para aplicaciones en dispositivos móviles y embebidos con recursos computacionales limitados.\n",
    "- Es útil en casos donde se necesita una red ligera y rápida, como en aplicaciones de visión por computadora en dispositivos móviles.\n",
    "- Útil para tareas de clasificación de imágenes, detección de objetos en tiempo real y otras aplicaciones de visión en dispositivos móviles.\n",
    "\n",
    "\n",
    "d. EfficientNet\n",
    "\n",
    "- EfficientNet es una familia de arquitecturas de CNN que buscan optimizar el equilibrio entre el rendimiento y la eficiencia computacional mediante el uso de escalado compuesto.\n",
    "- Es útil en casos donde se desean modelos con un buen rendimiento pero que sean escalables en términos de tamaño y requisitos computacionales.\n",
    "- Útil para una variedad de tareas de visión por computadora, desde clasificación de imágenes hasta detección de objetos y segmentación semántica.\n",
    "\n",
    "¿Cómo la arquitectura de transformers puede ser usada para image recognition?\n",
    "\n",
    "La arquitectura de Transformers se puede usar en el reconocimiento de imágenes al tratar las imágenes como secuencias de parches y aplicar mecanismos de atención y transformación para capturar información espacial y contextual. Esto se puede hacer al obtener las características, el uso de atención multi-cabeza y agregar información. Los modelos de visión Transformer se entrenan en conjuntos de datos etiquetados, se ajustan finamente en tareas específicas y permiten la transferencia de aprendizaje."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
