{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manuel Archila 161250\n",
    "##### Diego Franco 20240\n",
    "##### Juan Diego Avila 20090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Pr√°ctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.290354\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.315570\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.299078\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.298903\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.300027\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.312724\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.307347\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.314235\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.299103\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.297892\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300854\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.304880\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295555\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.306973\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.288721\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.300674\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.300735\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.296987\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.297271\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.303181\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.297223\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.307899\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.305202\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.299099\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.278854\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.305628\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.292988\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.284897\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.301391\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.292528\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.300167\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.318235\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.282169\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.308214\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.293639\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.287900\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.289978\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.289389\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.282530\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.280149\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.293365\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.291999\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.289127\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.285772\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.282268\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.272134\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.267790\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.270984\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.259552\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.279252\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.265924\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.263482\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.271164\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.255219\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.227378\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.241599\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.201749\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.225660\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.214654\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.207725\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.238866\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.173890\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.147619\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.129619\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.081799\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.104464\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.122292\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.079769\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.041947\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.935064\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.980747\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.821163\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.860492\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.967731\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.738807\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.842051\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.723595\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.634346\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.704315\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.696681\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.585786\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.460944\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.504637\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.538492\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.371103\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.488982\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.280876\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.339621\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.340497\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.295983\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.223771\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.143815\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.285403\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.114302\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.140909\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.214866\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.105295\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.114371\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.957275\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.113677\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.016166\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.836244\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.917692\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.962960\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.914223\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.935577\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.927864\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.908553\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.985946\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.797681\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.778887\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.921462\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.739085\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.915735\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.749777\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.927942\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.868798\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.705947\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.867870\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.877646\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.821957\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.677227\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.888611\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.587170\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.953355\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.589124\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.667878\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.710181\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.603574\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.790573\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.381692\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.691773\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.644380\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.597041\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.514326\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.674853\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.462263\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.619597\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.621748\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.470695\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.660009\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.599284\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.528506\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.608190\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.532200\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.468184\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.483214\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.643663\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.454628\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.467120\n",
      "(87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Definici√≥n del modelo\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(F.avg_pool2d(self.conv1(x), 2))\n",
    "        x = torch.tanh(F.avg_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Hiperpar√°metros\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 0.01\n",
    "\n",
    "# Transformaciones y carga del dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instancia del modelo, funci√≥n de p√©rdida y optimizador\n",
    "model = LeNet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)} \"\n",
    "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluaci√≥n\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "print(f\"({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Loss: 2.302589490895381\n",
      "Epoch 2/30, Loss: 2.3025704125309234\n",
      "Epoch 3/30, Loss: 2.3025254858729176\n",
      "Epoch 4/30, Loss: 2.302226912944823\n",
      "Epoch 5/30, Loss: 2.25119497983352\n",
      "Epoch 6/30, Loss: 2.213179388619445\n",
      "Epoch 7/30, Loss: 2.1572559105465783\n",
      "Epoch 8/30, Loss: 2.111590341533846\n",
      "Epoch 9/30, Loss: 2.071355868635885\n",
      "Epoch 10/30, Loss: 2.041391796163281\n",
      "Epoch 11/30, Loss: 1.9850449238896675\n",
      "Epoch 12/30, Loss: 1.9442216006996076\n",
      "Epoch 13/30, Loss: 1.9191369117068513\n",
      "Epoch 14/30, Loss: 1.886217120815726\n",
      "Epoch 15/30, Loss: 1.8580890584479817\n",
      "Epoch 16/30, Loss: 1.833253474948961\n",
      "Epoch 17/30, Loss: 1.8073494653872517\n",
      "Epoch 18/30, Loss: 1.7937313595696178\n",
      "Epoch 19/30, Loss: 1.7717730893808252\n",
      "Epoch 20/30, Loss: 1.7616859060114303\n",
      "Epoch 21/30, Loss: 1.758428267048448\n",
      "Epoch 22/30, Loss: 1.7453365409770585\n",
      "Epoch 23/30, Loss: 1.732283755031693\n",
      "Epoch 24/30, Loss: 1.710803150216027\n",
      "Epoch 25/30, Loss: 1.705536770546223\n",
      "Epoch 26/30, Loss: 1.6985302739740942\n",
      "Epoch 27/30, Loss: 1.695783346967624\n",
      "Epoch 28/30, Loss: 1.6854584367988665\n",
      "Epoch 29/30, Loss: 1.678006345658656\n",
      "Epoch 30/30, Loss: 1.6738292587077832\n",
      "Accuracy on test set: 75.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # Capa convolucional 1\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.lrn1 = nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capa convolucional 2\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capa convolucional 3\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Capa convolucional 4\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Capa convolucional 5\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Capas completamente conectadas\n",
    "        self.fc6 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc8 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.lrn1(self.relu1(self.conv1(x))))\n",
    "        x = self.pool2(self.lrn2(self.relu2(self.conv2(x))))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool5(self.relu5(self.conv5(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        \n",
    "        x = self.dropout6(self.relu6(self.fc6(x)))\n",
    "        x = self.dropout7(self.relu7(self.fc7(x)))\n",
    "        \n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Funci√≥n para entrenar el modelo\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=30):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Funci√≥n para evaluar el modelo\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy}%\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ¬øCu√°l es la diferencia principal entre ambas arquitecturas?\n",
    "\n",
    "- LaNet-5: es una arquitectura m√°s simple, contando con 2 capas convolucionales y 3 capas completamente conectadas. Se utiliza la funci√≥n de ctiaci√≥n de tanh y existe un menor n√∫mero de par√°metros y menor profundidad que logra AlexNet.\n",
    "- AlexNet: es una arquitectura m√°s compleja, que posee 5 capas convolucionales y 3 capas completamente conectadas. Utiliza la funci√≥n de activaci√≥n ReLU e implementa funciones como dropout para evitar el sobreajuste. Permite mayor profundiad y tiene un mayor n√∫mero de par√°metros que LaNet-5.\n",
    "\n",
    "b. Podr√≠a usarse LeNet-5 para un problema como el que resolvi√≥ usando AlexNet? ¬øY viceversa?\n",
    "- Tecnicamente LeNet-5 si se podr√≠a usar para problemas que resolvio Alexnet. Sin embargo LeNet-5 tiene un menor capaciddad y profundidad haciendo que no funcione tan bien como Alexnet. Por otro lado, Alexnet si se podr√≠a usar para problemas que resolvio LeNet-5, pero al tener una mayor capacidad y profundidad. Esta mayor capacidad puede hace que se use un modelo m√°s complejo para problrmas m√°s sencillos, lo cual puede llevar a un uso innecesario de recursos.\n",
    "\n",
    "c. Indique claramente qu√© le pareci√≥ m√°s interesante de cada arquitectura\n",
    "- Lo que m√°s nos llam√≥ la antenci√≥n de LeNet-5 es la simplicidad del modelo. A pesar de ser considerablemente simple, logr√≥ obtener resultados bastante buenos. Por otro lado, lo que m√°s nos llam√≥ la atenci√≥n de Alexnet es la complejidad del modelo. A pesar de ser un modelo complejo, no logr√≥ resultados tan buenos como LeNet-5. Esto nos hace pensar que la complejidad de un modelo no necesariamente se traduce en mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigue e indique en qu√© casos son √∫tiles las siguientes arquitecturas, agregue imagenes si esto le ayuda a una mejor comprensi√≥n\n",
    "\n",
    "a. GoogleNet (Inception)\n",
    "\n",
    "- GoogleNet, tambi√©n conocida como Inception, es una arquitectura de CNN desarrollada por Google. Se destac√≥ por su profundidad y eficiencia en la utilizaci√≥n de los recursos.\n",
    "- Es √∫til en casos donde se requieren redes profundas pero se desea mantener un uso eficiente de los recursos computacionales. GoogleNet utiliza una estructura llamada \"m√≥dulos Inception\" que combina m√∫ltiples tama√±os de filtros de convoluci√≥n en paralelo, permitiendo la extracci√≥n de caracter√≠sticas a diferentes escalas.\n",
    "- √ötil para tareas de clasificaci√≥n de im√°genes, detecci√≥n de objetos y segmentaci√≥n sem√°ntica.\n",
    "\n",
    "b. DenseNet (Densely Connected Convolutional Networks)\n",
    "\n",
    "- DenseNet es una arquitectura de CNN que se caracteriza por su densa conectividad entre capas. Cada capa est√° conectada directamente con todas las capas subsiguientes.\n",
    "- Es √∫til en casos donde se desea un mejor flujo de informaci√≥n y gradientes m√°s fuertes a lo largo de la red, lo que facilita el entrenamiento de redes profundas.\n",
    "- √ötil para tareas de clasificaci√≥n de im√°genes, detecci√≥n de objetos y segmentaci√≥n sem√°ntica.\n",
    "\n",
    "c. MobileNet\n",
    "\n",
    "- MobileNet es una arquitectura de CNN dise√±ada para aplicaciones en dispositivos m√≥viles y embebidos con recursos computacionales limitados.\n",
    "- Es √∫til en casos donde se necesita una red ligera y r√°pida, como en aplicaciones de visi√≥n por computadora en dispositivos m√≥viles.\n",
    "- √ötil para tareas de clasificaci√≥n de im√°genes, detecci√≥n de objetos en tiempo real y otras aplicaciones de visi√≥n en dispositivos m√≥viles.\n",
    "\n",
    "\n",
    "d. EfficientNet\n",
    "\n",
    "- EfficientNet es una familia de arquitecturas de CNN que buscan optimizar el equilibrio entre el rendimiento y la eficiencia computacional mediante el uso de escalado compuesto.\n",
    "- Es √∫til en casos donde se desean modelos con un buen rendimiento pero que sean escalables en t√©rminos de tama√±o y requisitos computacionales.\n",
    "- √ötil para una variedad de tareas de visi√≥n por computadora, desde clasificaci√≥n de im√°genes hasta detecci√≥n de objetos y segmentaci√≥n sem√°ntica.\n",
    "\n",
    "¬øC√≥mo la arquitectura de transformers puede ser usada para image recognition?\n",
    "\n",
    "La arquitectura de Transformers se puede usar en el reconocimiento de im√°genes al tratar las im√°genes como secuencias de parches y aplicar mecanismos de atenci√≥n y transformaci√≥n para capturar informaci√≥n espacial y contextual. Esto se puede hacer al obtener las caracter√≠sticas, el uso de atenci√≥n multi-cabeza y agregar informaci√≥n. Los modelos de visi√≥n Transformer se entrenan en conjuntos de datos etiquetados, se ajustan finamente en tareas espec√≠ficas y permiten la transferencia de aprendizaje."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
